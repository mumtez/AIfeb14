{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T20:04:42.873494Z",
     "start_time": "2025-03-01T20:04:39.853526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generated by Chat GPT\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "id": "5eea7d6d8ff9eb6f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T20:04:48.649136Z",
     "start_time": "2025-03-01T20:04:48.645703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generated by Chat GPT\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = [os.path.join(root_dir, f) for f in os.listdir(root_dir)\n",
    "                            if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n"
   ],
   "id": "63d85021acc54a67",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T20:06:49.784873Z",
     "start_time": "2025-03-01T20:06:49.780865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generated by Chat GPT\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Replace 'path_to_your_images' with your dataset folder path\n",
    "dataset = CustomImageDataset(root_dir='./archive/pokemon_jpg/pokemon_jpg/', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0)\n"
   ],
   "id": "35269cdf9e1fd34d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T20:06:50.267697Z",
     "start_time": "2025-03-01T20:06:50.264234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generated by Chat GPT\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        self.mask_type = mask_type\n",
    "        # Create mask same shape as weight\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        _, _, kh, kw = self.weight.size()\n",
    "        self.mask.fill_(1)\n",
    "        yc, xc = kh // 2, kw // 2\n",
    "\n",
    "        if mask_type == 'A':\n",
    "            # For the first layer, mask the center pixel and pixels to the right/below\n",
    "            self.mask[:, :, yc, xc:] = 0\n",
    "            self.mask[:, :, yc+1:] = 0\n",
    "        else:  # mask_type == 'B'\n",
    "            # For subsequent layers, allow the center pixel but mask future pixels\n",
    "            self.mask[:, :, yc, xc+1:] = 0\n",
    "            self.mask[:, :, yc+1:] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)\n"
   ],
   "id": "59f1172ade97551e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T20:06:50.936702Z",
     "start_time": "2025-03-01T20:06:50.933310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generated by Chat GPT\n",
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, input_channels=3, n_filters=64, kernel_size=7, n_layers=7, n_classes=256):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.net = nn.ModuleList()\n",
    "        # First layer with Mask-A convolution\n",
    "        self.net.append(MaskedConv2d('A', input_channels, n_filters, kernel_size, padding=kernel_size//2))\n",
    "        self.net.append(nn.ReLU())\n",
    "\n",
    "        # Subsequent layers with Mask-B convolutions\n",
    "        for _ in range(n_layers):\n",
    "            self.net.append(MaskedConv2d('B', n_filters, n_filters, kernel_size, padding=kernel_size//2))\n",
    "            self.net.append(nn.ReLU())\n",
    "\n",
    "        # Final 1x1 convolution to output logits for each class per channel\n",
    "        self.out_conv = nn.Conv2d(n_filters, input_channels * n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.net:\n",
    "            x = layer(x)\n",
    "        x = self.out_conv(x)\n",
    "        # Reshape to [B, input_channels, n_classes, H, W]\n",
    "        B, _, H, W = x.size()\n",
    "        x = x.view(B, self.input_channels, self.n_classes, H, W)\n",
    "        return x\n"
   ],
   "id": "d02654448d085372",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T20:09:35.226157Z",
     "start_time": "2025-03-01T20:07:53.097487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generated by Chat GPT\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PixelCNN(input_channels=3).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 100  # Adjust the number of epochs as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images in dataloader:\n",
    "        images = images.to(device)\n",
    "        # Quantize the pixel values (0-255) for each channel as target labels\n",
    "        targets = (images * 255).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # [B, 3, 256, H, W]\n",
    "        # Rearrange outputs to [B*3*H*W, 256] and targets to [B*3*H*W]\n",
    "        outputs = outputs.permute(0, 1, 3, 4, 2).contiguous().view(-1, model.n_classes)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(dataloader):.4f}\")\n"
   ],
   "id": "353cd0435ff1fa14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 4.4498\n",
      "Epoch [2/100], Loss: 3.4672\n",
      "Epoch [3/100], Loss: 3.3665\n",
      "Epoch [4/100], Loss: 3.2625\n",
      "Epoch [5/100], Loss: 3.1411\n",
      "Epoch [6/100], Loss: 3.0935\n",
      "Epoch [7/100], Loss: 3.0144\n",
      "Epoch [8/100], Loss: 2.9181\n",
      "Epoch [9/100], Loss: 2.8546\n",
      "Epoch [10/100], Loss: 2.8175\n",
      "Epoch [11/100], Loss: 2.8227\n",
      "Epoch [12/100], Loss: 2.8266\n",
      "Epoch [13/100], Loss: 2.7651\n",
      "Epoch [14/100], Loss: 2.7829\n",
      "Epoch [15/100], Loss: 2.7291\n",
      "Epoch [16/100], Loss: 2.7792\n",
      "Epoch [17/100], Loss: 2.7418\n",
      "Epoch [18/100], Loss: 2.6898\n",
      "Epoch [19/100], Loss: 2.6698\n",
      "Epoch [20/100], Loss: 2.7181\n",
      "Epoch [21/100], Loss: 2.8240\n",
      "Epoch [22/100], Loss: 2.6825\n",
      "Epoch [23/100], Loss: 2.6629\n",
      "Epoch [24/100], Loss: 2.6468\n",
      "Epoch [25/100], Loss: 2.6263\n",
      "Epoch [26/100], Loss: 2.6812\n",
      "Epoch [27/100], Loss: 2.6664\n",
      "Epoch [28/100], Loss: 2.6915\n",
      "Epoch [29/100], Loss: 2.6859\n",
      "Epoch [30/100], Loss: 2.6816\n",
      "Epoch [31/100], Loss: 2.6122\n",
      "Epoch [32/100], Loss: 2.6331\n",
      "Epoch [33/100], Loss: 2.6451\n",
      "Epoch [34/100], Loss: 2.5849\n",
      "Epoch [35/100], Loss: 2.6887\n",
      "Epoch [36/100], Loss: 2.6179\n",
      "Epoch [37/100], Loss: 2.5771\n",
      "Epoch [38/100], Loss: 2.5813\n",
      "Epoch [39/100], Loss: 2.6401\n",
      "Epoch [40/100], Loss: 2.5835\n",
      "Epoch [41/100], Loss: 2.5979\n",
      "Epoch [42/100], Loss: 2.5751\n",
      "Epoch [43/100], Loss: 2.6390\n",
      "Epoch [44/100], Loss: 2.5823\n",
      "Epoch [45/100], Loss: 2.5690\n",
      "Epoch [46/100], Loss: 2.5755\n",
      "Epoch [47/100], Loss: 2.5599\n",
      "Epoch [48/100], Loss: 2.6532\n",
      "Epoch [49/100], Loss: 2.5807\n",
      "Epoch [50/100], Loss: 2.6105\n",
      "Epoch [51/100], Loss: 2.5724\n",
      "Epoch [52/100], Loss: 2.5527\n",
      "Epoch [53/100], Loss: 2.5688\n",
      "Epoch [54/100], Loss: 2.6266\n",
      "Epoch [55/100], Loss: 2.5807\n",
      "Epoch [56/100], Loss: 2.5485\n",
      "Epoch [57/100], Loss: 2.5446\n",
      "Epoch [58/100], Loss: 2.6309\n",
      "Epoch [59/100], Loss: 2.5664\n",
      "Epoch [60/100], Loss: 2.5688\n",
      "Epoch [61/100], Loss: 2.7021\n",
      "Epoch [62/100], Loss: 2.6228\n",
      "Epoch [63/100], Loss: 2.5902\n",
      "Epoch [64/100], Loss: 2.5494\n",
      "Epoch [65/100], Loss: 2.6304\n",
      "Epoch [66/100], Loss: 2.5554\n",
      "Epoch [67/100], Loss: 2.5984\n",
      "Epoch [68/100], Loss: 2.5916\n",
      "Epoch [69/100], Loss: 2.5487\n",
      "Epoch [70/100], Loss: 2.5417\n",
      "Epoch [71/100], Loss: 2.5543\n",
      "Epoch [72/100], Loss: 2.5858\n",
      "Epoch [73/100], Loss: 2.6262\n",
      "Epoch [74/100], Loss: 2.6325\n",
      "Epoch [75/100], Loss: 2.5513\n",
      "Epoch [76/100], Loss: 2.5442\n",
      "Epoch [77/100], Loss: 2.5583\n",
      "Epoch [78/100], Loss: 2.5429\n",
      "Epoch [79/100], Loss: 2.5334\n",
      "Epoch [80/100], Loss: 3.1711\n",
      "Epoch [81/100], Loss: 2.7610\n",
      "Epoch [82/100], Loss: 2.7628\n",
      "Epoch [83/100], Loss: 2.6576\n",
      "Epoch [84/100], Loss: 2.6299\n",
      "Epoch [85/100], Loss: 2.6145\n",
      "Epoch [86/100], Loss: 2.5783\n",
      "Epoch [87/100], Loss: 2.5806\n",
      "Epoch [88/100], Loss: 2.5911\n",
      "Epoch [89/100], Loss: 2.5808\n",
      "Epoch [90/100], Loss: 2.5628\n",
      "Epoch [91/100], Loss: 2.5816\n",
      "Epoch [92/100], Loss: 2.5921\n",
      "Epoch [93/100], Loss: 2.5651\n",
      "Epoch [94/100], Loss: 2.5559\n",
      "Epoch [95/100], Loss: 2.5468\n",
      "Epoch [96/100], Loss: 2.5472\n",
      "Epoch [97/100], Loss: 2.6584\n",
      "Epoch [98/100], Loss: 2.5608\n",
      "Epoch [99/100], Loss: 2.5672\n",
      "Epoch [100/100], Loss: 2.5406\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T20:09:36.572291Z",
     "start_time": "2025-03-01T20:09:36.560579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generated by Chat GPT\n",
    "model_path = 'pixelcnn.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(\"Model saved.\")\n"
   ],
   "id": "8a1dc10524cae6d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T20:09:37.231180Z",
     "start_time": "2025-03-01T20:09:37.214950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generated by Chat GPT\n",
    "loaded_model = PixelCNN(input_channels=3).to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "loaded_model.eval()\n",
    "print(\"Model loaded.\")\n"
   ],
   "id": "5e8b951559c14d5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew\\AppData\\Local\\Temp\\ipykernel_24568\\2359939336.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T20:09:49.907431Z",
     "start_time": "2025-03-01T20:09:49.064127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generated by Chat GPT\n",
    "def sample(model, image_size=32, n_channels=3):\n",
    "    model.eval()\n",
    "    # Initialize an empty image tensor\n",
    "    sample_img = torch.zeros(1, n_channels, image_size, image_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(image_size):\n",
    "            for j in range(image_size):\n",
    "                outputs = model(sample_img)  # [1, 3, 256, H, W]\n",
    "                # For each channel, sample the pixel value\n",
    "                for c in range(n_channels):\n",
    "                    logits = outputs[0, c, :, i, j]  # [256]\n",
    "                    probs = torch.softmax(logits, dim=0)\n",
    "                    sampled_val = torch.multinomial(probs, 1)\n",
    "                    # Set the pixel value (normalize back to [0,1])\n",
    "                    sample_img[0, c, i, j] = sampled_val.float() / 255.0\n",
    "    return sample_img\n",
    "\n",
    "# Generate a sample image\n",
    "generated_image = sample(loaded_model, image_size=32, n_channels=3)\n"
   ],
   "id": "db3f01f0c6f89e64",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T20:09:50.127572Z",
     "start_time": "2025-03-01T20:09:50.104611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generated by Chat GPT\n",
    "# Convert tensor to numpy format and display\n",
    "img_np = generated_image.cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(img_np)\n",
    "plt.axis('off')\n",
    "plt.title(\"Generated Image\")\n",
    "plt.show()\n"
   ],
   "id": "14307869a787f0da",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This model performs similarly to the vae for Pokemon generation. I chose this because I was not able to implement a diffusion transformer. It was very easy to implement.",
   "id": "71a6abc749751e91"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
