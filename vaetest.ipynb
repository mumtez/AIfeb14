{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-15T01:52:26.857670Z",
     "start_time": "2025-02-15T01:52:26.854726Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T01:52:26.871584Z",
     "start_time": "2025-02-15T01:52:26.864565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the CSV file (adjust the filename if necessary)\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "print(df.head())\n",
    "\n",
    "# Combine the desired columns (Name, Email, School) into one string per record.\n",
    "def combine_fields(row):\n",
    "    return f\"{row['Name']}|{row['Email']}|{row['School']}\"\n",
    "\n",
    "data = df.apply(combine_fields, axis=1).tolist()\n",
    "\n",
    "# Build a character-level vocabulary from the data\n",
    "all_text = \"\\n\".join(data)\n",
    "chars = sorted(list(set(all_text)))\n",
    "print(f\"Found {len(chars)} unique characters.\")\n",
    "\n",
    "# Create mappings from characters to indices and back\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Add special tokens for start-of-sequence, end-of-sequence, and padding.\n",
    "special_tokens = [\"<SOS>\", \"<EOS>\", \"<PAD>\"]\n",
    "for token in special_tokens:\n",
    "    if token not in char2idx:\n",
    "        idx = len(char2idx)\n",
    "        char2idx[token] = idx\n",
    "        idx2char[idx] = token\n",
    "\n",
    "vocab_size = len(char2idx)\n",
    "print(\"Vocabulary size (including special tokens):\", vocab_size)\n"
   ],
   "id": "d9e3afc9857e8de4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Name  \\\n",
      "0  Dr. Robert M. Gamper, Ed. D.   \n",
      "1           Julianne Huettinger   \n",
      "2                 Robert Wright   \n",
      "3             Grace Biancorosso   \n",
      "4               Krista Kersting   \n",
      "\n",
      "                                               Title  \\\n",
      "0                          Superintendent of Schools   \n",
      "1     Administrative Assistant to the Superintendent   \n",
      "2             Business Administrator/Board Secretary   \n",
      "3  Administrative Assistant to the Business Admin...   \n",
      "4                   Assistant Business Administrator   \n",
      "\n",
      "                                    Email         School  \n",
      "0        robertgamper@parkridge.k12.nj.us  Park Ridge HS  \n",
      "1  juliannehuettinger@parkridge.k12.nj.us  Park Ridge HS  \n",
      "2        robertwright@parkridge.k12.nj.us  Park Ridge HS  \n",
      "3    gracebiancorosso@parkridge.k12.nj.us  Park Ridge HS  \n",
      "4      kristakersting@parkridge.k12.nj.us  Park Ridge HS  \n",
      "Found 63 unique characters.\n",
      "Vocabulary size (including special tokens): 66\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T01:52:26.890499Z",
     "start_time": "2025-02-15T01:52:26.886437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, char2idx):\n",
    "        self.texts = texts\n",
    "        self.char2idx = char2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        # Prepend <SOS> and append <EOS>\n",
    "        tokens = [\"<SOS>\"] + list(text) + [\"<EOS>\"]\n",
    "        indices = [self.char2idx.get(token, self.char2idx[\"<PAD>\"]) for token in tokens]\n",
    "        # For reconstruction: input is tokens[:-1] and target is tokens[1:]\n",
    "        input_tensor = torch.tensor(indices[:-1], dtype=torch.long)\n",
    "        target_tensor = torch.tensor(indices[1:], dtype=torch.long)\n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=char2idx[\"<PAD>\"])\n",
    "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=char2idx[\"<PAD>\"])\n",
    "    return inputs_padded.to(device), targets_padded.to(device)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(data, char2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n"
   ],
   "id": "7ed80b8d1d47f281",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T02:00:01.658860Z",
     "start_time": "2025-02-15T02:00:01.649364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "latent_dim = 2\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Encoder LSTM\n",
    "        self.encoder_rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden_to_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.hidden_to_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder LSTM\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.outputs_to_vocab = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (h_n, _) = self.encoder_rnn(embedded)\n",
    "        h_n = h_n.squeeze(0)\n",
    "        mu = self.hidden_to_mu(h_n)\n",
    "        logvar = self.hidden_to_logvar(h_n)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, targets=None, max_length=100, teacher_forcing_ratio=0.5):\n",
    "        batch_size = z.size(0)\n",
    "        hidden = self.latent_to_hidden(z).unsqueeze(0)\n",
    "        cell = torch.zeros_like(hidden).to(device)\n",
    "        input_token = torch.full((batch_size, 1), char2idx[\"<SOS>\"], dtype=torch.long).to(device)\n",
    "        outputs = []\n",
    "        for t in range(max_length):\n",
    "            embedded = self.embedding(input_token)\n",
    "            output, (hidden, cell) = self.decoder_rnn(embedded, (hidden, cell))\n",
    "            logits = self.outputs_to_vocab(output.squeeze(1))\n",
    "            outputs.append(logits.unsqueeze(1))\n",
    "            if targets is not None and random.random() < teacher_forcing_ratio and t < targets.size(1):\n",
    "                input_token = targets[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                input_token = logits.argmax(dim=1).unsqueeze(1)\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "    def forward(self, x, targets=None, teacher_forcing_ratio=0.5):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        max_length = targets.size(1) if targets is not None else 100\n",
    "        outputs = self.decode(z, targets, max_length, teacher_forcing_ratio)\n",
    "        return outputs, mu, logvar\n",
    "\n",
    "model = VAE(vocab_size, embedding_dim, hidden_dim, latent_dim).to(device)\n"
   ],
   "id": "9316563fa76d845b",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T02:03:10.043080Z",
     "start_time": "2025-02-15T02:00:02.798770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def loss_fn(recon_logits, target, mu, logvar):\n",
    "    # Reconstruction loss: cross-entropy ignoring padding\n",
    "    recon_loss = nn.functional.cross_entropy(\n",
    "        recon_logits.view(-1, recon_logits.size(-1)),\n",
    "        target.view(-1),\n",
    "        ignore_index=char2idx[\"<PAD>\"]\n",
    "    )\n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / mu.size(0)\n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 1000  # Adjust epochs as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs, mu, logvar = model(inputs, targets, teacher_forcing_ratio=0.5)\n",
    "        loss = loss_fn(outputs, targets, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n"
   ],
   "id": "a2ce746f89cfa4cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 - Loss: 4.1495\n",
      "Epoch 2/1000 - Loss: 3.6121\n",
      "Epoch 3/1000 - Loss: 3.4016\n",
      "Epoch 4/1000 - Loss: 3.3183\n",
      "Epoch 5/1000 - Loss: 3.2661\n",
      "Epoch 6/1000 - Loss: 3.2028\n",
      "Epoch 7/1000 - Loss: 3.1046\n",
      "Epoch 8/1000 - Loss: 3.0501\n",
      "Epoch 9/1000 - Loss: 3.0287\n",
      "Epoch 10/1000 - Loss: 2.9439\n",
      "Epoch 11/1000 - Loss: 2.9087\n",
      "Epoch 12/1000 - Loss: 2.8265\n",
      "Epoch 13/1000 - Loss: 2.7771\n",
      "Epoch 14/1000 - Loss: 2.7773\n",
      "Epoch 15/1000 - Loss: 2.7415\n",
      "Epoch 16/1000 - Loss: 2.6018\n",
      "Epoch 17/1000 - Loss: 2.5933\n",
      "Epoch 18/1000 - Loss: 2.5844\n",
      "Epoch 19/1000 - Loss: 2.4463\n",
      "Epoch 20/1000 - Loss: 2.4581\n",
      "Epoch 21/1000 - Loss: 2.4388\n",
      "Epoch 22/1000 - Loss: 2.3522\n",
      "Epoch 23/1000 - Loss: 2.3040\n",
      "Epoch 24/1000 - Loss: 2.2114\n",
      "Epoch 25/1000 - Loss: 2.1653\n",
      "Epoch 26/1000 - Loss: 2.1230\n",
      "Epoch 27/1000 - Loss: 2.1812\n",
      "Epoch 28/1000 - Loss: 2.0778\n",
      "Epoch 29/1000 - Loss: 2.0028\n",
      "Epoch 30/1000 - Loss: 2.0962\n",
      "Epoch 31/1000 - Loss: 2.1389\n",
      "Epoch 32/1000 - Loss: 2.0977\n",
      "Epoch 33/1000 - Loss: 2.0624\n",
      "Epoch 34/1000 - Loss: 1.9116\n",
      "Epoch 35/1000 - Loss: 1.9170\n",
      "Epoch 36/1000 - Loss: 1.9339\n",
      "Epoch 37/1000 - Loss: 1.8658\n",
      "Epoch 38/1000 - Loss: 1.8847\n",
      "Epoch 39/1000 - Loss: 1.8218\n",
      "Epoch 40/1000 - Loss: 1.7743\n",
      "Epoch 41/1000 - Loss: 1.9105\n",
      "Epoch 42/1000 - Loss: 1.7850\n",
      "Epoch 43/1000 - Loss: 1.7508\n",
      "Epoch 44/1000 - Loss: 1.7576\n",
      "Epoch 45/1000 - Loss: 1.7715\n",
      "Epoch 46/1000 - Loss: 1.7404\n",
      "Epoch 47/1000 - Loss: 1.6386\n",
      "Epoch 48/1000 - Loss: 1.7856\n",
      "Epoch 49/1000 - Loss: 1.6351\n",
      "Epoch 50/1000 - Loss: 1.6851\n",
      "Epoch 51/1000 - Loss: 1.7070\n",
      "Epoch 52/1000 - Loss: 1.7501\n",
      "Epoch 53/1000 - Loss: 1.6133\n",
      "Epoch 54/1000 - Loss: 1.6868\n",
      "Epoch 55/1000 - Loss: 1.6481\n",
      "Epoch 56/1000 - Loss: 1.6486\n",
      "Epoch 57/1000 - Loss: 1.6487\n",
      "Epoch 58/1000 - Loss: 1.6127\n",
      "Epoch 59/1000 - Loss: 1.5651\n",
      "Epoch 60/1000 - Loss: 1.6178\n",
      "Epoch 61/1000 - Loss: 1.5624\n",
      "Epoch 62/1000 - Loss: 1.4691\n",
      "Epoch 63/1000 - Loss: 1.5784\n",
      "Epoch 64/1000 - Loss: 1.5582\n",
      "Epoch 65/1000 - Loss: 1.5269\n",
      "Epoch 66/1000 - Loss: 1.5240\n",
      "Epoch 67/1000 - Loss: 1.4501\n",
      "Epoch 68/1000 - Loss: 1.4713\n",
      "Epoch 69/1000 - Loss: 1.4273\n",
      "Epoch 70/1000 - Loss: 1.5763\n",
      "Epoch 71/1000 - Loss: 1.6154\n",
      "Epoch 72/1000 - Loss: 1.5699\n",
      "Epoch 73/1000 - Loss: 1.5716\n",
      "Epoch 74/1000 - Loss: 1.4247\n",
      "Epoch 75/1000 - Loss: 1.5404\n",
      "Epoch 76/1000 - Loss: 1.4196\n",
      "Epoch 77/1000 - Loss: 1.4675\n",
      "Epoch 78/1000 - Loss: 1.3911\n",
      "Epoch 79/1000 - Loss: 1.4891\n",
      "Epoch 80/1000 - Loss: 1.4819\n",
      "Epoch 81/1000 - Loss: 1.3206\n",
      "Epoch 82/1000 - Loss: 1.4566\n",
      "Epoch 83/1000 - Loss: 1.3590\n",
      "Epoch 84/1000 - Loss: 1.4230\n",
      "Epoch 85/1000 - Loss: 1.4108\n",
      "Epoch 86/1000 - Loss: 1.5076\n",
      "Epoch 87/1000 - Loss: 1.4492\n",
      "Epoch 88/1000 - Loss: 1.3969\n",
      "Epoch 89/1000 - Loss: 1.2975\n",
      "Epoch 90/1000 - Loss: 1.5093\n",
      "Epoch 91/1000 - Loss: 1.4627\n",
      "Epoch 92/1000 - Loss: 1.3601\n",
      "Epoch 93/1000 - Loss: 1.4162\n",
      "Epoch 94/1000 - Loss: 1.3487\n",
      "Epoch 95/1000 - Loss: 1.4242\n",
      "Epoch 96/1000 - Loss: 1.3739\n",
      "Epoch 97/1000 - Loss: 1.3972\n",
      "Epoch 98/1000 - Loss: 1.2284\n",
      "Epoch 99/1000 - Loss: 1.5235\n",
      "Epoch 100/1000 - Loss: 1.3642\n",
      "Epoch 101/1000 - Loss: 1.2087\n",
      "Epoch 102/1000 - Loss: 1.5518\n",
      "Epoch 103/1000 - Loss: 1.1997\n",
      "Epoch 104/1000 - Loss: 1.3551\n",
      "Epoch 105/1000 - Loss: 1.5126\n",
      "Epoch 106/1000 - Loss: 1.3128\n",
      "Epoch 107/1000 - Loss: 1.3024\n",
      "Epoch 108/1000 - Loss: 1.4066\n",
      "Epoch 109/1000 - Loss: 1.4128\n",
      "Epoch 110/1000 - Loss: 1.3620\n",
      "Epoch 111/1000 - Loss: 1.2562\n",
      "Epoch 112/1000 - Loss: 1.4964\n",
      "Epoch 113/1000 - Loss: 1.2258\n",
      "Epoch 114/1000 - Loss: 1.4527\n",
      "Epoch 115/1000 - Loss: 1.1692\n",
      "Epoch 116/1000 - Loss: 1.1825\n",
      "Epoch 117/1000 - Loss: 1.1188\n",
      "Epoch 118/1000 - Loss: 1.1413\n",
      "Epoch 119/1000 - Loss: 1.3016\n",
      "Epoch 120/1000 - Loss: 1.3655\n",
      "Epoch 121/1000 - Loss: 1.2184\n",
      "Epoch 122/1000 - Loss: 1.2740\n",
      "Epoch 123/1000 - Loss: 1.1998\n",
      "Epoch 124/1000 - Loss: 1.2100\n",
      "Epoch 125/1000 - Loss: 1.2024\n",
      "Epoch 126/1000 - Loss: 1.1359\n",
      "Epoch 127/1000 - Loss: 1.2274\n",
      "Epoch 128/1000 - Loss: 1.2131\n",
      "Epoch 129/1000 - Loss: 1.1462\n",
      "Epoch 130/1000 - Loss: 1.2336\n",
      "Epoch 131/1000 - Loss: 1.2085\n",
      "Epoch 132/1000 - Loss: 1.1632\n",
      "Epoch 133/1000 - Loss: 1.1834\n",
      "Epoch 134/1000 - Loss: 1.2128\n",
      "Epoch 135/1000 - Loss: 1.0902\n",
      "Epoch 136/1000 - Loss: 1.1583\n",
      "Epoch 137/1000 - Loss: 1.2321\n",
      "Epoch 138/1000 - Loss: 1.0215\n",
      "Epoch 139/1000 - Loss: 1.2797\n",
      "Epoch 140/1000 - Loss: 1.0899\n",
      "Epoch 141/1000 - Loss: 1.1252\n",
      "Epoch 142/1000 - Loss: 0.9772\n",
      "Epoch 143/1000 - Loss: 1.0851\n",
      "Epoch 144/1000 - Loss: 1.2228\n",
      "Epoch 145/1000 - Loss: 1.2050\n",
      "Epoch 146/1000 - Loss: 1.1542\n",
      "Epoch 147/1000 - Loss: 0.9851\n",
      "Epoch 148/1000 - Loss: 1.1140\n",
      "Epoch 149/1000 - Loss: 1.0543\n",
      "Epoch 150/1000 - Loss: 1.0767\n",
      "Epoch 151/1000 - Loss: 1.1527\n",
      "Epoch 152/1000 - Loss: 1.0585\n",
      "Epoch 153/1000 - Loss: 1.2744\n",
      "Epoch 154/1000 - Loss: 1.0940\n",
      "Epoch 155/1000 - Loss: 1.1080\n",
      "Epoch 156/1000 - Loss: 1.0130\n",
      "Epoch 157/1000 - Loss: 1.0038\n",
      "Epoch 158/1000 - Loss: 1.0900\n",
      "Epoch 159/1000 - Loss: 1.0859\n",
      "Epoch 160/1000 - Loss: 0.9603\n",
      "Epoch 161/1000 - Loss: 1.1931\n",
      "Epoch 162/1000 - Loss: 0.9978\n",
      "Epoch 163/1000 - Loss: 0.9544\n",
      "Epoch 164/1000 - Loss: 0.9797\n",
      "Epoch 165/1000 - Loss: 0.8615\n",
      "Epoch 166/1000 - Loss: 0.8677\n",
      "Epoch 167/1000 - Loss: 0.9520\n",
      "Epoch 168/1000 - Loss: 0.8945\n",
      "Epoch 169/1000 - Loss: 1.1668\n",
      "Epoch 170/1000 - Loss: 0.9538\n",
      "Epoch 171/1000 - Loss: 0.9152\n",
      "Epoch 172/1000 - Loss: 0.9043\n",
      "Epoch 173/1000 - Loss: 1.0910\n",
      "Epoch 174/1000 - Loss: 0.9767\n",
      "Epoch 175/1000 - Loss: 0.8666\n",
      "Epoch 176/1000 - Loss: 0.9406\n",
      "Epoch 177/1000 - Loss: 0.9849\n",
      "Epoch 178/1000 - Loss: 1.0151\n",
      "Epoch 179/1000 - Loss: 0.9253\n",
      "Epoch 180/1000 - Loss: 0.9195\n",
      "Epoch 181/1000 - Loss: 1.0413\n",
      "Epoch 182/1000 - Loss: 0.8226\n",
      "Epoch 183/1000 - Loss: 1.0277\n",
      "Epoch 184/1000 - Loss: 0.8639\n",
      "Epoch 185/1000 - Loss: 0.9140\n",
      "Epoch 186/1000 - Loss: 0.7896\n",
      "Epoch 187/1000 - Loss: 0.7553\n",
      "Epoch 188/1000 - Loss: 0.8906\n",
      "Epoch 189/1000 - Loss: 0.7577\n",
      "Epoch 190/1000 - Loss: 1.1360\n",
      "Epoch 191/1000 - Loss: 0.7500\n",
      "Epoch 192/1000 - Loss: 0.7942\n",
      "Epoch 193/1000 - Loss: 0.9505\n",
      "Epoch 194/1000 - Loss: 0.9098\n",
      "Epoch 195/1000 - Loss: 0.8770\n",
      "Epoch 196/1000 - Loss: 0.7668\n",
      "Epoch 197/1000 - Loss: 0.8126\n",
      "Epoch 198/1000 - Loss: 0.9556\n",
      "Epoch 199/1000 - Loss: 0.8473\n",
      "Epoch 200/1000 - Loss: 0.7516\n",
      "Epoch 201/1000 - Loss: 0.8406\n",
      "Epoch 202/1000 - Loss: 0.9130\n",
      "Epoch 203/1000 - Loss: 0.7170\n",
      "Epoch 204/1000 - Loss: 0.9082\n",
      "Epoch 205/1000 - Loss: 0.8360\n",
      "Epoch 206/1000 - Loss: 0.9821\n",
      "Epoch 207/1000 - Loss: 0.5780\n",
      "Epoch 208/1000 - Loss: 0.5738\n",
      "Epoch 209/1000 - Loss: 0.7490\n",
      "Epoch 210/1000 - Loss: 1.0169\n",
      "Epoch 211/1000 - Loss: 0.6606\n",
      "Epoch 212/1000 - Loss: 0.7400\n",
      "Epoch 213/1000 - Loss: 0.7046\n",
      "Epoch 214/1000 - Loss: 0.7014\n",
      "Epoch 215/1000 - Loss: 0.8228\n",
      "Epoch 216/1000 - Loss: 0.6864\n",
      "Epoch 217/1000 - Loss: 0.9305\n",
      "Epoch 218/1000 - Loss: 0.8771\n",
      "Epoch 219/1000 - Loss: 0.6397\n",
      "Epoch 220/1000 - Loss: 0.7204\n",
      "Epoch 221/1000 - Loss: 0.8075\n",
      "Epoch 222/1000 - Loss: 0.6951\n",
      "Epoch 223/1000 - Loss: 0.6472\n",
      "Epoch 224/1000 - Loss: 0.7950\n",
      "Epoch 225/1000 - Loss: 0.9266\n",
      "Epoch 226/1000 - Loss: 0.6295\n",
      "Epoch 227/1000 - Loss: 0.5609\n",
      "Epoch 228/1000 - Loss: 0.9284\n",
      "Epoch 229/1000 - Loss: 0.7311\n",
      "Epoch 230/1000 - Loss: 0.5718\n",
      "Epoch 231/1000 - Loss: 0.6548\n",
      "Epoch 232/1000 - Loss: 0.5694\n",
      "Epoch 233/1000 - Loss: 0.7476\n",
      "Epoch 234/1000 - Loss: 0.5995\n",
      "Epoch 235/1000 - Loss: 0.5471\n",
      "Epoch 236/1000 - Loss: 0.7368\n",
      "Epoch 237/1000 - Loss: 0.8991\n",
      "Epoch 238/1000 - Loss: 0.5902\n",
      "Epoch 239/1000 - Loss: 0.6626\n",
      "Epoch 240/1000 - Loss: 0.5948\n",
      "Epoch 241/1000 - Loss: 0.9754\n",
      "Epoch 242/1000 - Loss: 0.9115\n",
      "Epoch 243/1000 - Loss: 0.8779\n",
      "Epoch 244/1000 - Loss: 0.6191\n",
      "Epoch 245/1000 - Loss: 0.6447\n",
      "Epoch 246/1000 - Loss: 0.5905\n",
      "Epoch 247/1000 - Loss: 0.6516\n",
      "Epoch 248/1000 - Loss: 0.6279\n",
      "Epoch 249/1000 - Loss: 0.5301\n",
      "Epoch 250/1000 - Loss: 0.6143\n",
      "Epoch 251/1000 - Loss: 0.4511\n",
      "Epoch 252/1000 - Loss: 0.6142\n",
      "Epoch 253/1000 - Loss: 0.4778\n",
      "Epoch 254/1000 - Loss: 0.7445\n",
      "Epoch 255/1000 - Loss: 0.5771\n",
      "Epoch 256/1000 - Loss: 0.6467\n",
      "Epoch 257/1000 - Loss: 0.5268\n",
      "Epoch 258/1000 - Loss: 0.5057\n",
      "Epoch 259/1000 - Loss: 0.4596\n",
      "Epoch 260/1000 - Loss: 0.5821\n",
      "Epoch 261/1000 - Loss: 0.5877\n",
      "Epoch 262/1000 - Loss: 0.7303\n",
      "Epoch 263/1000 - Loss: 0.4757\n",
      "Epoch 264/1000 - Loss: 0.5515\n",
      "Epoch 265/1000 - Loss: 0.6124\n",
      "Epoch 266/1000 - Loss: 0.4133\n",
      "Epoch 267/1000 - Loss: 0.6271\n",
      "Epoch 268/1000 - Loss: 0.5938\n",
      "Epoch 269/1000 - Loss: 0.4503\n",
      "Epoch 270/1000 - Loss: 0.3370\n",
      "Epoch 271/1000 - Loss: 0.4373\n",
      "Epoch 272/1000 - Loss: 0.5670\n",
      "Epoch 273/1000 - Loss: 0.3976\n",
      "Epoch 274/1000 - Loss: 0.4782\n",
      "Epoch 275/1000 - Loss: 0.5417\n",
      "Epoch 276/1000 - Loss: 0.5318\n",
      "Epoch 277/1000 - Loss: 0.6397\n",
      "Epoch 278/1000 - Loss: 0.5009\n",
      "Epoch 279/1000 - Loss: 0.5843\n",
      "Epoch 280/1000 - Loss: 0.6121\n",
      "Epoch 281/1000 - Loss: 0.4308\n",
      "Epoch 282/1000 - Loss: 0.6716\n",
      "Epoch 283/1000 - Loss: 0.5001\n",
      "Epoch 284/1000 - Loss: 0.4151\n",
      "Epoch 285/1000 - Loss: 0.4240\n",
      "Epoch 286/1000 - Loss: 0.6166\n",
      "Epoch 287/1000 - Loss: 0.5212\n",
      "Epoch 288/1000 - Loss: 0.6045\n",
      "Epoch 289/1000 - Loss: 0.4707\n",
      "Epoch 290/1000 - Loss: 0.4340\n",
      "Epoch 291/1000 - Loss: 0.3969\n",
      "Epoch 292/1000 - Loss: 0.4711\n",
      "Epoch 293/1000 - Loss: 0.5651\n",
      "Epoch 294/1000 - Loss: 0.5765\n",
      "Epoch 295/1000 - Loss: 0.6633\n",
      "Epoch 296/1000 - Loss: 0.3121\n",
      "Epoch 297/1000 - Loss: 0.3432\n",
      "Epoch 298/1000 - Loss: 0.6456\n",
      "Epoch 299/1000 - Loss: 0.6535\n",
      "Epoch 300/1000 - Loss: 0.7534\n",
      "Epoch 301/1000 - Loss: 0.5264\n",
      "Epoch 302/1000 - Loss: 0.4842\n",
      "Epoch 303/1000 - Loss: 0.7899\n",
      "Epoch 304/1000 - Loss: 0.4740\n",
      "Epoch 305/1000 - Loss: 0.2633\n",
      "Epoch 306/1000 - Loss: 0.4775\n",
      "Epoch 307/1000 - Loss: 0.4492\n",
      "Epoch 308/1000 - Loss: 0.5995\n",
      "Epoch 309/1000 - Loss: 0.4193\n",
      "Epoch 310/1000 - Loss: 0.4389\n",
      "Epoch 311/1000 - Loss: 0.3299\n",
      "Epoch 312/1000 - Loss: 0.3876\n",
      "Epoch 313/1000 - Loss: 0.6257\n",
      "Epoch 314/1000 - Loss: 0.4584\n",
      "Epoch 315/1000 - Loss: 0.3489\n",
      "Epoch 316/1000 - Loss: 0.3389\n",
      "Epoch 317/1000 - Loss: 0.4329\n",
      "Epoch 318/1000 - Loss: 0.5274\n",
      "Epoch 319/1000 - Loss: 0.6719\n",
      "Epoch 320/1000 - Loss: 0.5600\n",
      "Epoch 321/1000 - Loss: 0.5377\n",
      "Epoch 322/1000 - Loss: 0.3132\n",
      "Epoch 323/1000 - Loss: 0.4012\n",
      "Epoch 324/1000 - Loss: 0.3162\n",
      "Epoch 325/1000 - Loss: 0.4432\n",
      "Epoch 326/1000 - Loss: 0.6216\n",
      "Epoch 327/1000 - Loss: 0.5573\n",
      "Epoch 328/1000 - Loss: 0.3631\n",
      "Epoch 329/1000 - Loss: 0.3519\n",
      "Epoch 330/1000 - Loss: 0.3850\n",
      "Epoch 331/1000 - Loss: 0.4079\n",
      "Epoch 332/1000 - Loss: 0.8999\n",
      "Epoch 333/1000 - Loss: 0.4443\n",
      "Epoch 334/1000 - Loss: 0.6039\n",
      "Epoch 335/1000 - Loss: 0.5495\n",
      "Epoch 336/1000 - Loss: 0.4531\n",
      "Epoch 337/1000 - Loss: 0.4430\n",
      "Epoch 338/1000 - Loss: 0.5296\n",
      "Epoch 339/1000 - Loss: 0.4284\n",
      "Epoch 340/1000 - Loss: 0.6895\n",
      "Epoch 341/1000 - Loss: 0.4990\n",
      "Epoch 342/1000 - Loss: 0.4931\n",
      "Epoch 343/1000 - Loss: 0.5654\n",
      "Epoch 344/1000 - Loss: 0.6467\n",
      "Epoch 345/1000 - Loss: 0.4915\n",
      "Epoch 346/1000 - Loss: 0.4732\n",
      "Epoch 347/1000 - Loss: 0.6626\n",
      "Epoch 348/1000 - Loss: 0.4783\n",
      "Epoch 349/1000 - Loss: 0.5671\n",
      "Epoch 350/1000 - Loss: 0.4814\n",
      "Epoch 351/1000 - Loss: 0.4927\n",
      "Epoch 352/1000 - Loss: 0.4917\n",
      "Epoch 353/1000 - Loss: 0.3786\n",
      "Epoch 354/1000 - Loss: 0.3896\n",
      "Epoch 355/1000 - Loss: 0.3472\n",
      "Epoch 356/1000 - Loss: 0.3473\n",
      "Epoch 357/1000 - Loss: 0.9803\n",
      "Epoch 358/1000 - Loss: 0.4185\n",
      "Epoch 359/1000 - Loss: 0.5724\n",
      "Epoch 360/1000 - Loss: 0.4333\n",
      "Epoch 361/1000 - Loss: 0.2759\n",
      "Epoch 362/1000 - Loss: 0.3457\n",
      "Epoch 363/1000 - Loss: 0.3955\n",
      "Epoch 364/1000 - Loss: 0.4044\n",
      "Epoch 365/1000 - Loss: 0.6910\n",
      "Epoch 366/1000 - Loss: 0.4831\n",
      "Epoch 367/1000 - Loss: 0.4878\n",
      "Epoch 368/1000 - Loss: 0.5326\n",
      "Epoch 369/1000 - Loss: 0.3079\n",
      "Epoch 370/1000 - Loss: 0.5827\n",
      "Epoch 371/1000 - Loss: 0.2845\n",
      "Epoch 372/1000 - Loss: 0.5142\n",
      "Epoch 373/1000 - Loss: 0.4319\n",
      "Epoch 374/1000 - Loss: 0.3652\n",
      "Epoch 375/1000 - Loss: 0.5699\n",
      "Epoch 376/1000 - Loss: 0.5105\n",
      "Epoch 377/1000 - Loss: 0.6834\n",
      "Epoch 378/1000 - Loss: 0.5909\n",
      "Epoch 379/1000 - Loss: 0.5678\n",
      "Epoch 380/1000 - Loss: 0.3175\n",
      "Epoch 381/1000 - Loss: 0.4288\n",
      "Epoch 382/1000 - Loss: 0.4236\n",
      "Epoch 383/1000 - Loss: 0.3725\n",
      "Epoch 384/1000 - Loss: 0.3386\n",
      "Epoch 385/1000 - Loss: 0.4756\n",
      "Epoch 386/1000 - Loss: 0.3814\n",
      "Epoch 387/1000 - Loss: 0.3521\n",
      "Epoch 388/1000 - Loss: 0.5207\n",
      "Epoch 389/1000 - Loss: 0.2935\n",
      "Epoch 390/1000 - Loss: 0.5515\n",
      "Epoch 391/1000 - Loss: 0.6024\n",
      "Epoch 392/1000 - Loss: 0.3464\n",
      "Epoch 393/1000 - Loss: 0.2918\n",
      "Epoch 394/1000 - Loss: 0.5152\n",
      "Epoch 395/1000 - Loss: 0.6221\n",
      "Epoch 396/1000 - Loss: 0.4280\n",
      "Epoch 397/1000 - Loss: 0.6176\n",
      "Epoch 398/1000 - Loss: 0.4353\n",
      "Epoch 399/1000 - Loss: 0.4698\n",
      "Epoch 400/1000 - Loss: 0.5536\n",
      "Epoch 401/1000 - Loss: 0.6075\n",
      "Epoch 402/1000 - Loss: 0.3807\n",
      "Epoch 403/1000 - Loss: 0.4287\n",
      "Epoch 404/1000 - Loss: 0.3677\n",
      "Epoch 405/1000 - Loss: 0.4463\n",
      "Epoch 406/1000 - Loss: 0.2957\n",
      "Epoch 407/1000 - Loss: 0.5098\n",
      "Epoch 408/1000 - Loss: 0.4245\n",
      "Epoch 409/1000 - Loss: 0.6699\n",
      "Epoch 410/1000 - Loss: 0.5516\n",
      "Epoch 411/1000 - Loss: 0.7028\n",
      "Epoch 412/1000 - Loss: 0.4003\n",
      "Epoch 413/1000 - Loss: 0.4324\n",
      "Epoch 414/1000 - Loss: 0.6471\n",
      "Epoch 415/1000 - Loss: 0.4798\n",
      "Epoch 416/1000 - Loss: 0.7927\n",
      "Epoch 417/1000 - Loss: 0.3231\n",
      "Epoch 418/1000 - Loss: 0.3773\n",
      "Epoch 419/1000 - Loss: 0.2664\n",
      "Epoch 420/1000 - Loss: 0.3462\n",
      "Epoch 421/1000 - Loss: 0.3689\n",
      "Epoch 422/1000 - Loss: 0.2909\n",
      "Epoch 423/1000 - Loss: 0.3043\n",
      "Epoch 424/1000 - Loss: 0.3473\n",
      "Epoch 425/1000 - Loss: 0.4405\n",
      "Epoch 426/1000 - Loss: 0.2875\n",
      "Epoch 427/1000 - Loss: 0.5088\n",
      "Epoch 428/1000 - Loss: 0.3451\n",
      "Epoch 429/1000 - Loss: 0.3839\n",
      "Epoch 430/1000 - Loss: 0.3281\n",
      "Epoch 431/1000 - Loss: 0.4827\n",
      "Epoch 432/1000 - Loss: 0.3337\n",
      "Epoch 433/1000 - Loss: 0.5343\n",
      "Epoch 434/1000 - Loss: 0.5016\n",
      "Epoch 435/1000 - Loss: 0.3766\n",
      "Epoch 436/1000 - Loss: 0.6344\n",
      "Epoch 437/1000 - Loss: 0.3945\n",
      "Epoch 438/1000 - Loss: 0.4127\n",
      "Epoch 439/1000 - Loss: 0.2862\n",
      "Epoch 440/1000 - Loss: 0.4144\n",
      "Epoch 441/1000 - Loss: 0.2625\n",
      "Epoch 442/1000 - Loss: 0.3164\n",
      "Epoch 443/1000 - Loss: 0.5678\n",
      "Epoch 444/1000 - Loss: 0.5021\n",
      "Epoch 445/1000 - Loss: 0.4118\n",
      "Epoch 446/1000 - Loss: 0.4223\n",
      "Epoch 447/1000 - Loss: 0.4115\n",
      "Epoch 448/1000 - Loss: 0.3662\n",
      "Epoch 449/1000 - Loss: 0.5164\n",
      "Epoch 450/1000 - Loss: 0.5050\n",
      "Epoch 451/1000 - Loss: 0.3336\n",
      "Epoch 452/1000 - Loss: 0.3477\n",
      "Epoch 453/1000 - Loss: 0.3172\n",
      "Epoch 454/1000 - Loss: 0.2922\n",
      "Epoch 455/1000 - Loss: 0.5371\n",
      "Epoch 456/1000 - Loss: 0.3487\n",
      "Epoch 457/1000 - Loss: 0.3173\n",
      "Epoch 458/1000 - Loss: 0.6725\n",
      "Epoch 459/1000 - Loss: 0.3791\n",
      "Epoch 460/1000 - Loss: 0.3376\n",
      "Epoch 461/1000 - Loss: 0.2725\n",
      "Epoch 462/1000 - Loss: 0.3828\n",
      "Epoch 463/1000 - Loss: 0.2935\n",
      "Epoch 464/1000 - Loss: 0.3036\n",
      "Epoch 465/1000 - Loss: 0.5089\n",
      "Epoch 466/1000 - Loss: 0.3660\n",
      "Epoch 467/1000 - Loss: 0.5122\n",
      "Epoch 468/1000 - Loss: 0.4605\n",
      "Epoch 469/1000 - Loss: 0.4884\n",
      "Epoch 470/1000 - Loss: 0.5305\n",
      "Epoch 471/1000 - Loss: 0.3488\n",
      "Epoch 472/1000 - Loss: 0.3276\n",
      "Epoch 473/1000 - Loss: 0.3669\n",
      "Epoch 474/1000 - Loss: 0.3662\n",
      "Epoch 475/1000 - Loss: 0.2573\n",
      "Epoch 476/1000 - Loss: 0.5382\n",
      "Epoch 477/1000 - Loss: 0.3887\n",
      "Epoch 478/1000 - Loss: 0.6248\n",
      "Epoch 479/1000 - Loss: 0.5321\n",
      "Epoch 480/1000 - Loss: 0.5024\n",
      "Epoch 481/1000 - Loss: 0.5103\n",
      "Epoch 482/1000 - Loss: 0.3535\n",
      "Epoch 483/1000 - Loss: 0.2399\n",
      "Epoch 484/1000 - Loss: 0.4823\n",
      "Epoch 485/1000 - Loss: 0.4687\n",
      "Epoch 486/1000 - Loss: 0.2874\n",
      "Epoch 487/1000 - Loss: 0.3432\n",
      "Epoch 488/1000 - Loss: 0.3641\n",
      "Epoch 489/1000 - Loss: 0.6169\n",
      "Epoch 490/1000 - Loss: 0.3208\n",
      "Epoch 491/1000 - Loss: 0.2626\n",
      "Epoch 492/1000 - Loss: 0.3627\n",
      "Epoch 493/1000 - Loss: 0.3687\n",
      "Epoch 494/1000 - Loss: 0.3514\n",
      "Epoch 495/1000 - Loss: 0.3108\n",
      "Epoch 496/1000 - Loss: 0.3604\n",
      "Epoch 497/1000 - Loss: 0.2959\n",
      "Epoch 498/1000 - Loss: 0.6433\n",
      "Epoch 499/1000 - Loss: 0.3401\n",
      "Epoch 500/1000 - Loss: 0.3966\n",
      "Epoch 501/1000 - Loss: 0.4713\n",
      "Epoch 502/1000 - Loss: 0.3478\n",
      "Epoch 503/1000 - Loss: 0.2998\n",
      "Epoch 504/1000 - Loss: 0.6218\n",
      "Epoch 505/1000 - Loss: 0.3188\n",
      "Epoch 506/1000 - Loss: 0.2436\n",
      "Epoch 507/1000 - Loss: 0.4350\n",
      "Epoch 508/1000 - Loss: 0.3293\n",
      "Epoch 509/1000 - Loss: 0.4453\n",
      "Epoch 510/1000 - Loss: 0.3569\n",
      "Epoch 511/1000 - Loss: 0.3070\n",
      "Epoch 512/1000 - Loss: 0.4195\n",
      "Epoch 513/1000 - Loss: 0.4950\n",
      "Epoch 514/1000 - Loss: 0.2992\n",
      "Epoch 515/1000 - Loss: 0.3790\n",
      "Epoch 516/1000 - Loss: 0.5403\n",
      "Epoch 517/1000 - Loss: 0.6363\n",
      "Epoch 518/1000 - Loss: 0.4195\n",
      "Epoch 519/1000 - Loss: 0.5440\n",
      "Epoch 520/1000 - Loss: 0.3478\n",
      "Epoch 521/1000 - Loss: 0.4846\n",
      "Epoch 522/1000 - Loss: 0.5000\n",
      "Epoch 523/1000 - Loss: 0.4869\n",
      "Epoch 524/1000 - Loss: 0.5846\n",
      "Epoch 525/1000 - Loss: 0.6738\n",
      "Epoch 526/1000 - Loss: 0.3660\n",
      "Epoch 527/1000 - Loss: 0.3260\n",
      "Epoch 528/1000 - Loss: 0.4705\n",
      "Epoch 529/1000 - Loss: 0.3480\n",
      "Epoch 530/1000 - Loss: 0.3931\n",
      "Epoch 531/1000 - Loss: 0.3412\n",
      "Epoch 532/1000 - Loss: 0.3455\n",
      "Epoch 533/1000 - Loss: 0.5195\n",
      "Epoch 534/1000 - Loss: 0.3499\n",
      "Epoch 535/1000 - Loss: 0.3120\n",
      "Epoch 536/1000 - Loss: 0.4322\n",
      "Epoch 537/1000 - Loss: 0.4670\n",
      "Epoch 538/1000 - Loss: 0.5450\n",
      "Epoch 539/1000 - Loss: 0.3557\n",
      "Epoch 540/1000 - Loss: 0.3632\n",
      "Epoch 541/1000 - Loss: 0.4257\n",
      "Epoch 542/1000 - Loss: 0.3357\n",
      "Epoch 543/1000 - Loss: 0.2955\n",
      "Epoch 544/1000 - Loss: 0.4419\n",
      "Epoch 545/1000 - Loss: 0.4525\n",
      "Epoch 546/1000 - Loss: 0.2956\n",
      "Epoch 547/1000 - Loss: 0.4551\n",
      "Epoch 548/1000 - Loss: 0.4484\n",
      "Epoch 549/1000 - Loss: 0.2830\n",
      "Epoch 550/1000 - Loss: 0.3586\n",
      "Epoch 551/1000 - Loss: 0.3813\n",
      "Epoch 552/1000 - Loss: 0.4962\n",
      "Epoch 553/1000 - Loss: 0.3572\n",
      "Epoch 554/1000 - Loss: 0.2884\n",
      "Epoch 555/1000 - Loss: 0.3799\n",
      "Epoch 556/1000 - Loss: 0.4440\n",
      "Epoch 557/1000 - Loss: 0.3665\n",
      "Epoch 558/1000 - Loss: 0.3629\n",
      "Epoch 559/1000 - Loss: 0.3866\n",
      "Epoch 560/1000 - Loss: 0.3835\n",
      "Epoch 561/1000 - Loss: 0.3468\n",
      "Epoch 562/1000 - Loss: 0.2693\n",
      "Epoch 563/1000 - Loss: 0.4857\n",
      "Epoch 564/1000 - Loss: 0.2936\n",
      "Epoch 565/1000 - Loss: 0.4495\n",
      "Epoch 566/1000 - Loss: 0.3775\n",
      "Epoch 567/1000 - Loss: 0.4100\n",
      "Epoch 568/1000 - Loss: 0.4509\n",
      "Epoch 569/1000 - Loss: 0.3704\n",
      "Epoch 570/1000 - Loss: 0.4701\n",
      "Epoch 571/1000 - Loss: 0.3831\n",
      "Epoch 572/1000 - Loss: 0.2653\n",
      "Epoch 573/1000 - Loss: 0.3136\n",
      "Epoch 574/1000 - Loss: 0.7019\n",
      "Epoch 575/1000 - Loss: 0.6079\n",
      "Epoch 576/1000 - Loss: 0.4528\n",
      "Epoch 577/1000 - Loss: 0.2295\n",
      "Epoch 578/1000 - Loss: 0.4512\n",
      "Epoch 579/1000 - Loss: 0.4483\n",
      "Epoch 580/1000 - Loss: 0.4753\n",
      "Epoch 581/1000 - Loss: 0.4228\n",
      "Epoch 582/1000 - Loss: 0.3493\n",
      "Epoch 583/1000 - Loss: 0.4035\n",
      "Epoch 584/1000 - Loss: 0.5615\n",
      "Epoch 585/1000 - Loss: 0.5044\n",
      "Epoch 586/1000 - Loss: 0.3111\n",
      "Epoch 587/1000 - Loss: 0.3274\n",
      "Epoch 588/1000 - Loss: 0.2968\n",
      "Epoch 589/1000 - Loss: 0.3420\n",
      "Epoch 590/1000 - Loss: 0.5921\n",
      "Epoch 591/1000 - Loss: 0.2870\n",
      "Epoch 592/1000 - Loss: 0.3960\n",
      "Epoch 593/1000 - Loss: 0.5066\n",
      "Epoch 594/1000 - Loss: 0.4693\n",
      "Epoch 595/1000 - Loss: 0.3059\n",
      "Epoch 596/1000 - Loss: 0.3801\n",
      "Epoch 597/1000 - Loss: 0.4587\n",
      "Epoch 598/1000 - Loss: 0.4996\n",
      "Epoch 599/1000 - Loss: 0.5206\n",
      "Epoch 600/1000 - Loss: 0.4308\n",
      "Epoch 601/1000 - Loss: 0.2618\n",
      "Epoch 602/1000 - Loss: 0.2734\n",
      "Epoch 603/1000 - Loss: 0.4449\n",
      "Epoch 604/1000 - Loss: 0.3821\n",
      "Epoch 605/1000 - Loss: 0.4732\n",
      "Epoch 606/1000 - Loss: 0.4216\n",
      "Epoch 607/1000 - Loss: 0.2665\n",
      "Epoch 608/1000 - Loss: 0.4679\n",
      "Epoch 609/1000 - Loss: 0.5144\n",
      "Epoch 610/1000 - Loss: 0.5802\n",
      "Epoch 611/1000 - Loss: 0.5079\n",
      "Epoch 612/1000 - Loss: 0.4642\n",
      "Epoch 613/1000 - Loss: 0.2554\n",
      "Epoch 614/1000 - Loss: 0.3521\n",
      "Epoch 615/1000 - Loss: 0.3377\n",
      "Epoch 616/1000 - Loss: 0.3917\n",
      "Epoch 617/1000 - Loss: 0.4263\n",
      "Epoch 618/1000 - Loss: 0.2716\n",
      "Epoch 619/1000 - Loss: 0.3008\n",
      "Epoch 620/1000 - Loss: 0.3129\n",
      "Epoch 621/1000 - Loss: 0.3105\n",
      "Epoch 622/1000 - Loss: 0.3144\n",
      "Epoch 623/1000 - Loss: 0.2456\n",
      "Epoch 624/1000 - Loss: 0.2925\n",
      "Epoch 625/1000 - Loss: 0.2452\n",
      "Epoch 626/1000 - Loss: 0.3320\n",
      "Epoch 627/1000 - Loss: 0.2900\n",
      "Epoch 628/1000 - Loss: 0.3743\n",
      "Epoch 629/1000 - Loss: 0.2814\n",
      "Epoch 630/1000 - Loss: 0.3979\n",
      "Epoch 631/1000 - Loss: 0.2763\n",
      "Epoch 632/1000 - Loss: 0.3586\n",
      "Epoch 633/1000 - Loss: 0.2819\n",
      "Epoch 634/1000 - Loss: 0.2227\n",
      "Epoch 635/1000 - Loss: 0.3887\n",
      "Epoch 636/1000 - Loss: 0.2231\n",
      "Epoch 637/1000 - Loss: 0.3770\n",
      "Epoch 638/1000 - Loss: 0.5042\n",
      "Epoch 639/1000 - Loss: 0.2801\n",
      "Epoch 640/1000 - Loss: 0.6023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 22\u001B[0m\n\u001B[0;32m     20\u001B[0m outputs, mu, logvar \u001B[38;5;241m=\u001B[39m model(inputs, targets, teacher_forcing_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n\u001B[0;32m     21\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(outputs, targets, mu, logvar)\n\u001B[1;32m---> 22\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     23\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     24\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\School_Projects\\Lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[0;32m    582\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[0;32m    583\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\School_Projects\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m _engine_run_backward(\n\u001B[0;32m    348\u001B[0m     tensors,\n\u001B[0;32m    349\u001B[0m     grad_tensors_,\n\u001B[0;32m    350\u001B[0m     retain_graph,\n\u001B[0;32m    351\u001B[0m     create_graph,\n\u001B[0;32m    352\u001B[0m     inputs,\n\u001B[0;32m    353\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    354\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    355\u001B[0m )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\School_Projects\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    827\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T02:03:11.653339Z",
     "start_time": "2025-02-15T02:03:11.638024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_samples(model, num_samples=5, max_length=100):\n",
    "    model.eval()\n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        outputs = model.decode(z, targets=None, max_length=max_length, teacher_forcing_ratio=0.0)\n",
    "        tokens = outputs.argmax(dim=-1)\n",
    "        for i in range(num_samples):\n",
    "            token_list = tokens[i].cpu().numpy().tolist()\n",
    "            generated = []\n",
    "            for token_idx in token_list:\n",
    "                token = idx2char[token_idx]\n",
    "                if token == \"<EOS>\":\n",
    "                    break\n",
    "                if token in [\"<SOS>\", \"<PAD>\"]:\n",
    "                    continue\n",
    "                generated.append(token)\n",
    "            samples.append(\"\".join(generated))\n",
    "    return samples\n",
    "\n",
    "# Display generated samples\n",
    "print(\"\\nGenerated Samples:\")\n",
    "for sample in generate_samples(model, num_samples=5, max_length=100):\n",
    "    print(sample)\n"
   ],
   "id": "3852b8b837ceb734",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Samples:\n",
      "Macha Goran|suramon@plps.org|Pompton Lakes\n",
      "Macha Goran|suramon@plps.org|Pompton Lakes\n",
      "Macha Goran|suramon@plps.org|Pompton Lakes\n",
      "Macha Goran|suramon@plps.org|Pompton Lakes\n",
      "Macha Goran|suramon@plps.org|Pompton Lakes\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T01:52:52.641476Z",
     "start_time": "2025-02-15T01:52:52.639441Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5dc4f4418f1fd5da",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
